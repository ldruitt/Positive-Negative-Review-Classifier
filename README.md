# Positive-Negative-Review-Classifier
Classifying positive and negative Amazon reviews with KNN and NLTK.
In order to classify the data, I found that its best to first preprocess the data. To do this I used the Natural Language Toolkit (NLTK) and regular expressions to take all the lines of text and normalize them by lemmatizing all the words (getting the root of the word), removing all special characters, turning all text to lower case, stripping extra white space, and then removing ‘stop words’ which are words that are used so commonly that they would only slow down the performance of the algorithm without adding anything of relevance. I also saved the classifying scores of each line of the training data in a separate structure so that they would not be modified. 
	Once the data is culled of extraneous characters and words, I was able to turn it into features. I decided the most important measure of each vector, in terms of positive and negative classification, was the sentiment behind each word in a line and the polarity scores of each line. I was able to calculate these using the NLTK Sentiment Intensity Analyzer. This gave me four features:  the positive, negative, neutral, and compound scores of each line.  Once these were calculated I was able compare the lines by these four numerical features that represent the sentiment in each line.
	To compare the data is essentially to get the numerical distance, which is the next step towards getting the nearest data points. In order to calculate the distances for all test points compared to every training point I needed to be extremely discerning with how I handled efficiency so as not to have an excessive overhead for the large amount of data. I ended up calculating L2 distance in a form that did not require any for-loops. I utilized Numpy’s ability to do math on arrays in order to calculate distance on the entire matrix of test points and train data at once. My equation was based on the L2 distance for 2 vectors: 
d2(I1,I2)=||I1−I2||=√||I1||2+||I2||2−2I1⋅I2
This resulted in a matrix of N (training points with known classifiers) x M (test points) distances.
	The final step in classifying the data is getting the K nearest neighbors for each test point. At this step I used the distance matrix knowing that each training point was a row and each test point was a column. I was able to handle each column (an array of distances from the test point to each train point) using Numpy’s sort function, instantaneously getting the points in order of distance. Once the column is sorted, I got the k nearest points by simply taking a slice of the array from 0 to k. The classifiers for these points are added to a structure and this process is done for every column. This results in a structure that holds the k nearest neighbors’ classifiers (+1 or -1) for every test point. To classify each test point I used Numpy’s sum function with the condition of elements being ’+1’. If the number of +1’s counted by Numpy in a group of neighbors was a majority of the group, the test point would be classified as +1, otherwise it would be classified as a -1. Each classification was then written into an output file.
	For cross validation this process is done solely on the training data which is split ten-fold, each fold alternatingly being used as test data and classified while the rest is used as training data. The accuracy score is calculated using the Numpy sum function with the condition of classifications of the classified points equaling the true classifications over the number of classifications made. Essentially:
Accuracy = 
This is done for all 10 folds and then divided by 10 to get the final accuracy score for all the folds. 
	For memory management I found that it was possible to run the test data in batches. I split the test data into 10 pieces and then used my algorithm comparing the test points of each batch to the entirety of the training data. While the data was still quite large this decreased the memory needed by decreasing the dimensions of the distance matrix being used by 90%. This allowed me to get all the test points distances within the scope of a realistic amount of RAM usage.
Methodology:
	My approach was largely based on trial and error. Originally, I was using Sci-kit’s TF-IDF model and feature extractor in order to create my features. This created a matrix of all the possible words and their frequency in each line. This was an interesting approach but even on just a small section of the data it created exponentially more features than there were lines to be compared. Even after preprocessing the data thoroughly (lemmatizing, removing stop words, etc.) this was far too many features to be processed. This is what led me to change my features to be a numerical value that held a true weight in terms of representation of the reviews and had a limited number of features per line i.e. the sentiment intensity analyzer from NLTK. 
	The implementation of the K-nearest algorithm was also a product of special consideration. Originally my code went through every single test point with for-loops, comparing that test point to every single training point thereby getting the distances by computing the Euclidean distance for 4 features (positive, negative, neutral, and compound scores) such that:
   dist((x,y,z,w),(a,b,c,d))=√(x - a)² + (y - b)² + (z – c)² + (w – d)²
and saving the best scores to a structure of K best scores. Upon exiting the algorithm, the code could then get the classification (+1 or -1) for a single test point by using a for loop that counted all the positive and negative ones in the neighbors for that point. The overhead for this approach wasn’t too bad for a small amount of the data but took over 7 hours to run for the entirety of the data. I found this to be unacceptable, so I attempted to optimize my implementation of the K-nearest neighbors. This led to my use of the vectorized comparison approach using Numpy’s ability to compute math on arrays. My new approach needed no for loops to compute distance for the entire body of data and used a sort and slice approach to get the k nearest points for one test point. In the end my algorithm used only one for loop to get K nearest neighbors for every test point and one to classify every test point. This new approach was able to run in under an hour.
	After several runs of cross-validation on training data I decided that k = 15, gave me the best accuracy from the k’s that I was able to try with L2 distance for vectors to calculate the similarity between vectors. 
	This new approach was great in terms of efficiency but required a lot of memory management for the full 18506 lines of test data and training data. To mitigate this, I simply decided to batch run my test data. Each test point is still compared to the entirety of the training data but now the dimensions of the distance matrices are reduced so that the RAM on my computer could better handle them. After all the modifications to my original code I felt confident that the algorithm was sufficiently optimized.
